import gymnasium as gym
from gymnasium.wrappers import AtariPreprocessing, RecordVideo

import numpy as np
from PPOConvNet import PPOConvAgent
from tqdm import tqdm
import wandb

import math

wandb.login()

counter = 0

def train(config):
    global counter
    env = gym.make("ALE/Tetris-v5", obs_type="grayscale", frameskip=1, render_mode='rgb_array')
    env = AtariPreprocessing(env, screen_size=84, scale_obs=True)
    env = RecordVideo(env, 
                      video_folder="/Users/christophermao/Documents/GitHub/SoccerAI/Videos", 
                      episode_trigger=lambda x: x % 500 == 0, 
                      video_length=0,
                      name_prefix=str(counter) + "")
    
    N = config.N                            # update agent every N steps: 20
    batch_size = config.batch_size          # Batch size: 5
    n_epochs = config.n_epochs              # n epochs: 10
    alpha = config.lr                       # lr: 0.0003
    n_games = 10_000                         # n games: 300

    fc_config={
        'conv1': config.conv1, 
        'conv2': config.conv1, 
        'linear': config.linear, 
        'kernal': config.kernal, 
        'stride': config.stride, 
        'padding': config.padding
    }


    agent = PPOConvAgent(input_size=env.observation_space.shape, n_actions=env.action_space.n, batch_size=batch_size, alpha=alpha, 
              n_epochs=n_epochs, fc_config=fc_config)

    best_score = env.reward_range[0]
    score_history = []

    learn_iters = 0
    avg_score = 0
    n_steps = 0
    agent.actor.train()
    agent.critic.train()
    prev_step = 0

    score_start=0.0001
    score_end = 0.001
    score_decay = 500

    for i in range(n_games):
        observation, _ = env.reset()
        done = False
        score = 0
        loss = []

        observation = [observation] 

        while not done:
            action, prob, val = agent.choose_action(observation)

            obs, reward, terminated, truncated, _ = env.step(action)
            reward += score_end + (score_start - score_end) * math.exp(-1. * (n_steps - prev_step) / score_decay)
            n_steps += 1
            score += reward
            done = terminated or truncated
            agent.remember(observation, action, prob, val, reward, done)


            if n_steps % N == 0: # update
                loss.append(agent.learn())
                learn_iters += 1
            observation = [obs]
        

        score_history.append(score)

        if (len(score_history) > 100):
            avg_score = np.mean(score_history[-100])
        
        if avg_score > best_score:
            best_score = avg_score
            agent.save_models(name="Tetris")

        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score, 
            'time_steps', n_steps - prev_step, 'learning_steps', learn_iters)
        wandb.log({
            'Score': score, 
            'Average Score': avg_score,
            'Episode Length': n_steps - prev_step,
            'Total Loss': sum(loss),
            'Average Loss': (sum(loss) / n_steps),
        })
        prev_step = n_steps

    
    final_reward = []
    for i in range(30):
        total_reward = []
        obs, _ = env.reset()
        while True:
            action = agent.choose_action(obs, train_mode=False)
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            total_reward.append(reward)
            if done:
                break
        final_reward.append(sum(total_reward))
    counter += 1
    return sum(final_reward) / 30

    

# Define the search space
sweep_configuration = {
    "method": "bayes",
    "metric": {
        "goal": "maximize",
        "name": 'Score'
    },
    "parameters": {
        "N": {
            "distribution": "int_uniform",
            "max": 200,
            "min": 50
        },
        "batch_size": {
            "distribution": "int_uniform",
            "max": 500,
            "min": 100,
        },
        "n_epochs": {
            "distribution": "int_uniform",
            "max": 10,
            "min": 1,
        },
        "lr": {
            "distribution": "uniform",
            "max": 0.0001,
            "min": 1e-07,
        },
        # "n_games": {
        #     "distribution": "int_uniform",
        #     "max": 10_000,
        #     "min": 1_000,
        # },
        "conv1": {
            "distribution": "int_uniform",
            "max": 64,
            "min": 16,
        },
        "conv2": {
            "distribution": "int_uniform",
            "max": 32,
            "min": 8,
        },
        "linear": {
            "distribution": "int_uniform",
            "max": 64,
            "min": 16,
        },
        "kernal": {
            "distribution": "int_uniform",
            "max": 4,
            "min": 3,
        },
        "stride": {
            "distribution": "int_uniform",
            "max": 4,
            "min": 3,
        },
        "padding": {
            "distribution": "int_uniform",
            "max": 1,
            "min": 0,
        },
    },
}

def main():
    wandb.init()
    score = train(wandb.config)
    wandb.log({"score": score})

# Start the sweep
sweep_id = wandb.sweep(sweep=sweep_configuration, project="Tetris")

wandb.agent(sweep_id, function=main, count=20)

# x = [i+1 for i in range(len(score_history))]
# plot_learning_curve(x, score_history, figure_file)